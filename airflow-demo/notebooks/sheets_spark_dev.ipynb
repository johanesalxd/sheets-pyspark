{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheets to BigQuery - Spark Development Version\n",
    "\n",
    "This notebook is designed for development and will be converted to a Python script for production execution on Dataproc Serverless.\n",
    "\n",
    "It reads data from Google Sheets and writes to BigQuery using Spark DataFrames.\n",
    "\n",
    "**Development:** Use this notebook for interactive development\n",
    "**Production:** DAG converts this to `.py` script for Dataproc Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - Dual mode: batch (sys.argv) or interactive (defaults)\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if running with command-line arguments (batch mode)\n",
    "if len(sys.argv) > 1:\n",
    "    # BATCH MODE: Read from command-line arguments passed by Dataproc\n",
    "    GCP_PROJECT = sys.argv[1]\n",
    "    GCP_REGION = sys.argv[2] if len(sys.argv) > 2 else \"us-central1\"\n",
    "    logger.info(f\"Batch mode: project={GCP_PROJECT}, region={GCP_REGION}\")\n",
    "else:\n",
    "    # INTERACTIVE MODE: Use defaults for development\n",
    "    GCP_PROJECT = \"johanesa-playground-326616\"  # UPDATE THIS for your project\n",
    "    GCP_REGION = \"us-central1\"\n",
    "    logger.info(f\"Interactive mode: project={GCP_PROJECT}, region={GCP_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "logger.info(f\"Starting Spark notebook execution for project: {GCP_PROJECT}\")\n",
    "\n",
    "# Credentials path in GCS\n",
    "credentials_path = f\"gs://{GCP_PROJECT}-notebooks/credentials/drive-api.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-conversion"
    ]
   },
   "outputs": [],
   "source": [
    "# INTERACTIVE MODE ONLY - This cell is skipped during notebook-to-script conversion\n",
    "# Create Spark session for Dataproc Serverless Interactive Session\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from google.cloud.dataproc_v1 import Session, SparkConnectConfig\n",
    "\n",
    "session_config = Session()\n",
    "session_config.spark_connect_session = SparkConnectConfig()\n",
    "# TODO: Replace with your actual session template\n",
    "session_config.session_template = f'projects/{GCP_PROJECT}/locations/{GCP_REGION}/sessionTemplates/runtime-00000b96da90'\n",
    "\n",
    "spark = DataprocSparkSession.builder \\\n",
    "    .projectId(GCP_PROJECT) \\\n",
    "    .location(GCP_REGION) \\\n",
    "    .dataprocSessionConfig(session_config) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(f\"Interactive Spark session created: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH MODE - Create standard Spark session for Dataproc Serverless batch\n",
    "# This cell runs in both modes but handles each case appropriately\n",
    "try:\n",
    "    # Check if spark session already exists (from interactive cell above)\n",
    "    spark\n",
    "    logger.info(\"Using existing Spark session (interactive mode)\")\n",
    "except NameError:\n",
    "    # Create new Spark session for batch mode\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"sheets-to-bigquery\") \\\n",
    "        .getOrCreate()\n",
    "    logger.info(f\"Batch Spark session created: {spark.version}\")\n",
    "\n",
    "logger.info(f\"BigQuery project: {GCP_PROJECT}\")\n",
    "logger.info(f\"BigQuery location: {GCP_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Sheets API\n",
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "# Download credentials from GCS to local file\n",
    "local_creds = \"drive-api.json\"\n",
    "if not os.path.exists(local_creds):\n",
    "    logger.info(f\"Downloading credentials from {credentials_path}\")\n",
    "\n",
    "    # Parse GCS path (gs://bucket-name/path/to/file)\n",
    "    bucket_name = f\"{GCP_PROJECT}-notebooks\"\n",
    "    blob_name = \"credentials/drive-api.json\"\n",
    "\n",
    "    # Download using Python GCS client\n",
    "    storage_client = storage.Client(project=GCP_PROJECT)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.download_to_filename(local_creds)\n",
    "\n",
    "    logger.info(\"Credentials downloaded successfully\")\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    local_creds, scope)\n",
    "googleClient = gspread.authorize(credentials)\n",
    "\n",
    "logger.info(\"Successfully authenticated with Google Sheets API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load legacy_charges data\n",
    "LEGACY_CHARGES_SHEET_ID = os.getenv(\n",
    "    \"LEGACY_CHARGES_SHEET_ID\",\n",
    "    \"1kQENu6sumzEQX60fjQtgmXvwPGlUfaNRgW7v_TWFUXo\"\n",
    ")\n",
    "\n",
    "sheet = googleClient.open_by_key(LEGACY_CHARGES_SHEET_ID)\n",
    "worksheet = sheet.get_worksheet(0)\n",
    "legacy_charges = worksheet.get_all_records(numericise_ignore=['all'])\n",
    "\n",
    "# Clean data\n",
    "for record in legacy_charges:\n",
    "    record['mid_label'] = None if record['mid_label'] == '' else record['mid_label']\n",
    "    record['installment_count'] = None if record['installment_count'] == '' else int(\n",
    "        record['installment_count'])\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "legacy_charges_df = spark.createDataFrame(legacy_charges).withColumn(\n",
    "    \"installment_count\", col(\"installment_count\").cast(\"int\"))\n",
    "\n",
    "logger.info(\n",
    "    f\"Successfully loaded {legacy_charges_df.count()} records from legacy_charges sheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant_send_mid_label data\n",
    "MERCHANT_SEND_MID_LABEL_SHEET_ID = os.getenv(\n",
    "    \"MERCHANT_SEND_MID_LABEL_SHEET_ID\",\n",
    "    \"1_8sm8QciAU3T8oDlNS1Pfj-GQlmlJBrAi1TYdnnMlkw\"\n",
    ")\n",
    "\n",
    "sheet = googleClient.open_by_key(MERCHANT_SEND_MID_LABEL_SHEET_ID)\n",
    "worksheet = sheet.get_worksheet(0)\n",
    "merchant_send_mid_label = worksheet.get_all_records()\n",
    "merchant_send_mid_label_df = spark.createDataFrame(merchant_send_mid_label)\n",
    "\n",
    "logger.info(\n",
    "    f\"Successfully loaded {merchant_send_mid_label_df.count()} records from merchant_send_mid_label sheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant_excluded data\n",
    "MERCHANT_EXCLUDED_SHEET_ID = os.getenv(\n",
    "    \"MERCHANT_EXCLUDED_SHEET_ID\",\n",
    "    \"1orVBlPP77HTt9d8x-lC1Oo5xrPp0r1FgVUQ-43DYqYc\"\n",
    ")\n",
    "\n",
    "sheet = googleClient.open_by_key(MERCHANT_EXCLUDED_SHEET_ID)\n",
    "worksheet = sheet.get_worksheet(0)\n",
    "merchant_excluded = worksheet.get_all_records()\n",
    "merchant_excluded_df = spark.createDataFrame(merchant_excluded)\n",
    "\n",
    "logger.info(\n",
    "    f\"Successfully loaded {merchant_excluded_df.count()} records from merchant_excluded sheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "legacy_charges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to BigQuery temp tables using Spark BigQuery connector\n",
    "legacy_charges_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{GCP_PROJECT}.temp.legacy_charges\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "merchant_send_mid_label_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{GCP_PROJECT}.temp.merchant_send_mid_label\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "merchant_excluded_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{GCP_PROJECT}.temp.merchant_excluded\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "logger.info(\"Successfully wrote all dataframes to BigQuery temp tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final results table using Spark SQL\n",
    "RESULTS_TABLE = os.getenv(\"RESULTS_TABLE\", \"temp.filtered_legacy_charges\")\n",
    "\n",
    "# Register temp views\n",
    "legacy_charges_df.createOrReplaceTempView(\"legacy_charges\")\n",
    "merchant_send_mid_label_df.createOrReplaceTempView(\"merchant_send_mid_label\")\n",
    "merchant_excluded_df.createOrReplaceTempView(\"merchant_excluded\")\n",
    "\n",
    "# Execute SQL query\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    lc.*,\n",
    "    CURRENT_TIMESTAMP() as processed_at\n",
    "FROM\n",
    "    legacy_charges lc\n",
    "LEFT JOIN\n",
    "    merchant_send_mid_label msml ON lc.business_id = msml.business_id\n",
    "LEFT JOIN\n",
    "    merchant_excluded me ON lc.business_id = me.business_id\n",
    "WHERE\n",
    "    msml.business_id IS NULL AND me.business_id IS NULL\n",
    "\"\"\")\n",
    "\n",
    "# Write results to BigQuery\n",
    "result_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{GCP_PROJECT}.{RESULTS_TABLE}\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "logger.info(f\"Successfully created table {RESULTS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row count and log completion\n",
    "count = result_df.count()\n",
    "\n",
    "logger.info(f\"Successfully created table {RESULTS_TABLE} with {count} records\")\n",
    "logger.info(\n",
    "    f\"View results: https://console.cloud.google.com/bigquery?project={GCP_PROJECT}&d=temp&t=filtered_legacy_charges&page=table\")\n",
    "logger.info(\"Notebook execution completed successfully\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
